{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    }
   },
   "outputs": [],
   "source": [
    "student1 = \"Nicolas Lesimple\"\n",
    "student2 = \"Nakka Krishna\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of classes selected: 1, 0, 5, 6, 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "# np.random.seed(hash(student1 + student2) % 2**32)\n",
    "np.random.seed(237699 + 279617)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)\n",
    "print(\"Subset of classes selected: {}\".format(', '.join([str(subset_of_classes[i]) for i in range(5)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullTrain data size: (73257, 256)\n",
      "Full Test data size: (26032, 256)\n"
     ]
    }
   ],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('housenumbers/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('housenumbers/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)\n",
    "print(\"FullTrain data size: {}\\nFull Test data size: {}\".format(x_train_all.shape,x_test_all.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We furter reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Train data size: (43226, 256)\n",
      "Sampled Test data size: (15767, 256)\n"
     ]
    }
   ],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)\n",
    "\n",
    "print(\"Sampled Train data size: {}\\nSampled Test data size: {}\".format(x_train.shape,x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_180 (Dense)            (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,285\n",
      "Trainable params: 1,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43226 samples, validate on 15767 samples\n",
      "Epoch 1/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5708 - acc: 0.3198 - val_loss: 1.5662 - val_acc: 0.3227\n",
      "Epoch 2/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5576 - acc: 0.3348 - val_loss: 1.5552 - val_acc: 0.3269\n",
      "Epoch 3/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5504 - acc: 0.3374 - val_loss: 1.5476 - val_acc: 0.3316\n",
      "Epoch 4/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5449 - acc: 0.3406 - val_loss: 1.5428 - val_acc: 0.3361\n",
      "Epoch 5/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5407 - acc: 0.3416 - val_loss: 1.5379 - val_acc: 0.3429\n",
      "Epoch 6/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5374 - acc: 0.3441 - val_loss: 1.5361 - val_acc: 0.3431\n",
      "Epoch 7/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5346 - acc: 0.3454 - val_loss: 1.5321 - val_acc: 0.3417\n",
      "Epoch 8/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5322 - acc: 0.3467 - val_loss: 1.5305 - val_acc: 0.3439\n",
      "Epoch 9/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5299 - acc: 0.3478 - val_loss: 1.5291 - val_acc: 0.3429\n",
      "Epoch 10/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5280 - acc: 0.3489 - val_loss: 1.5285 - val_acc: 0.3444\n",
      "Epoch 11/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5264 - acc: 0.3510 - val_loss: 1.5262 - val_acc: 0.3484\n",
      "Epoch 12/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5249 - acc: 0.3526 - val_loss: 1.5252 - val_acc: 0.3458\n",
      "Epoch 13/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5234 - acc: 0.3530 - val_loss: 1.5238 - val_acc: 0.3505\n",
      "Epoch 14/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5221 - acc: 0.3542 - val_loss: 1.5244 - val_acc: 0.3536\n",
      "Epoch 15/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5208 - acc: 0.3556 - val_loss: 1.5231 - val_acc: 0.3526\n",
      "Epoch 16/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5197 - acc: 0.3567 - val_loss: 1.5216 - val_acc: 0.3524\n",
      "Epoch 17/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5187 - acc: 0.3574 - val_loss: 1.5218 - val_acc: 0.3556\n",
      "Epoch 18/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5176 - acc: 0.3588 - val_loss: 1.5217 - val_acc: 0.3554\n",
      "Epoch 19/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5165 - acc: 0.3598 - val_loss: 1.5205 - val_acc: 0.3566\n",
      "Epoch 20/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5157 - acc: 0.3611 - val_loss: 1.5198 - val_acc: 0.3537\n",
      "Epoch 21/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5149 - acc: 0.3616 - val_loss: 1.5199 - val_acc: 0.3566\n",
      "Epoch 22/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5139 - acc: 0.3624 - val_loss: 1.5209 - val_acc: 0.3625\n",
      "Epoch 23/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5132 - acc: 0.3632 - val_loss: 1.5194 - val_acc: 0.3589\n",
      "Epoch 24/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5124 - acc: 0.3650 - val_loss: 1.5193 - val_acc: 0.3544\n",
      "Epoch 25/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5116 - acc: 0.3644 - val_loss: 1.5183 - val_acc: 0.3566\n",
      "Epoch 26/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5110 - acc: 0.3661 - val_loss: 1.5183 - val_acc: 0.3585\n",
      "Epoch 27/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5102 - acc: 0.3661 - val_loss: 1.5183 - val_acc: 0.3613\n",
      "Epoch 28/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5096 - acc: 0.3677 - val_loss: 1.5186 - val_acc: 0.3644\n",
      "Epoch 29/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5090 - acc: 0.3683 - val_loss: 1.5189 - val_acc: 0.3672\n",
      "Epoch 30/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5084 - acc: 0.3690 - val_loss: 1.5178 - val_acc: 0.3610\n",
      "Epoch 31/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5079 - acc: 0.3690 - val_loss: 1.5179 - val_acc: 0.3621\n",
      "Epoch 32/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5071 - acc: 0.3692 - val_loss: 1.5185 - val_acc: 0.3636\n",
      "Epoch 33/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5067 - acc: 0.3712 - val_loss: 1.5171 - val_acc: 0.3589\n",
      "Epoch 34/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5061 - acc: 0.3716 - val_loss: 1.5175 - val_acc: 0.3590\n",
      "Epoch 35/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5056 - acc: 0.3713 - val_loss: 1.5172 - val_acc: 0.3616\n",
      "Epoch 36/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5051 - acc: 0.3729 - val_loss: 1.5184 - val_acc: 0.3696\n",
      "Epoch 37/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5047 - acc: 0.3740 - val_loss: 1.5188 - val_acc: 0.3668\n",
      "Epoch 38/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5042 - acc: 0.3737 - val_loss: 1.5174 - val_acc: 0.3658\n",
      "Epoch 39/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5037 - acc: 0.3748 - val_loss: 1.5172 - val_acc: 0.3639\n",
      "Epoch 40/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5032 - acc: 0.3751 - val_loss: 1.5174 - val_acc: 0.3666\n",
      "Epoch 41/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5029 - acc: 0.3751 - val_loss: 1.5171 - val_acc: 0.3632\n",
      "Epoch 42/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5024 - acc: 0.3757 - val_loss: 1.5178 - val_acc: 0.3699\n",
      "Epoch 43/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5020 - acc: 0.3771 - val_loss: 1.5172 - val_acc: 0.3634\n",
      "Epoch 44/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5016 - acc: 0.3768 - val_loss: 1.5176 - val_acc: 0.3706\n",
      "Epoch 45/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5013 - acc: 0.3786 - val_loss: 1.5178 - val_acc: 0.3692\n",
      "Epoch 46/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5009 - acc: 0.3784 - val_loss: 1.5174 - val_acc: 0.3658\n",
      "Epoch 47/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5005 - acc: 0.3788 - val_loss: 1.5171 - val_acc: 0.3658\n",
      "Epoch 48/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.5001 - acc: 0.3787 - val_loss: 1.5176 - val_acc: 0.3695\n",
      "Epoch 49/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4998 - acc: 0.3793 - val_loss: 1.5171 - val_acc: 0.3689\n",
      "Epoch 50/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4994 - acc: 0.3804 - val_loss: 1.5172 - val_acc: 0.3701\n",
      "Epoch 51/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4992 - acc: 0.3808 - val_loss: 1.5175 - val_acc: 0.3665\n",
      "Epoch 52/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4988 - acc: 0.3804 - val_loss: 1.5178 - val_acc: 0.3713\n",
      "Epoch 53/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4986 - acc: 0.3814 - val_loss: 1.5175 - val_acc: 0.3711\n",
      "Epoch 54/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4981 - acc: 0.3816 - val_loss: 1.5171 - val_acc: 0.3690\n",
      "Epoch 55/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4978 - acc: 0.3824 - val_loss: 1.5169 - val_acc: 0.3676\n",
      "Epoch 56/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4976 - acc: 0.3829 - val_loss: 1.5175 - val_acc: 0.3706\n",
      "Epoch 57/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4974 - acc: 0.3832 - val_loss: 1.5176 - val_acc: 0.3717\n",
      "Epoch 58/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4970 - acc: 0.3836 - val_loss: 1.5173 - val_acc: 0.3695\n",
      "Epoch 59/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4968 - acc: 0.3837 - val_loss: 1.5185 - val_acc: 0.3706\n",
      "Epoch 60/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43226/43226 [==============================] - 0s - loss: 1.4965 - acc: 0.3839 - val_loss: 1.5174 - val_acc: 0.3703\n",
      "Epoch 61/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4963 - acc: 0.3847 - val_loss: 1.5181 - val_acc: 0.3727\n",
      "Epoch 62/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4959 - acc: 0.3851 - val_loss: 1.5185 - val_acc: 0.3660\n",
      "Epoch 63/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4957 - acc: 0.3857 - val_loss: 1.5175 - val_acc: 0.3702\n",
      "Epoch 64/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4954 - acc: 0.3851 - val_loss: 1.5184 - val_acc: 0.3745\n",
      "Epoch 65/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4952 - acc: 0.3861 - val_loss: 1.5185 - val_acc: 0.3710\n",
      "Epoch 66/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4950 - acc: 0.3859 - val_loss: 1.5173 - val_acc: 0.3708\n",
      "Epoch 67/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4948 - acc: 0.3860 - val_loss: 1.5176 - val_acc: 0.3717\n",
      "Epoch 68/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4946 - acc: 0.3872 - val_loss: 1.5178 - val_acc: 0.3724\n",
      "Epoch 69/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4943 - acc: 0.3873 - val_loss: 1.5181 - val_acc: 0.3691\n",
      "Epoch 70/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4941 - acc: 0.3874 - val_loss: 1.5176 - val_acc: 0.3704\n",
      "Epoch 71/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4939 - acc: 0.3880 - val_loss: 1.5188 - val_acc: 0.3732\n",
      "Epoch 72/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4936 - acc: 0.3885 - val_loss: 1.5177 - val_acc: 0.3712\n",
      "Epoch 73/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4935 - acc: 0.3881 - val_loss: 1.5180 - val_acc: 0.3717\n",
      "Epoch 74/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4931 - acc: 0.3883 - val_loss: 1.5186 - val_acc: 0.3738\n",
      "Epoch 75/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4930 - acc: 0.3878 - val_loss: 1.5185 - val_acc: 0.3746\n",
      "Epoch 76/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4928 - acc: 0.3886 - val_loss: 1.5185 - val_acc: 0.3757\n",
      "Epoch 77/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4926 - acc: 0.3894 - val_loss: 1.5189 - val_acc: 0.3718\n",
      "Epoch 78/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4925 - acc: 0.3897 - val_loss: 1.5185 - val_acc: 0.3732\n",
      "Epoch 79/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4923 - acc: 0.3893 - val_loss: 1.5190 - val_acc: 0.3733\n",
      "Epoch 80/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4920 - acc: 0.3895 - val_loss: 1.5200 - val_acc: 0.3756\n",
      "Epoch 81/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4919 - acc: 0.3897 - val_loss: 1.5196 - val_acc: 0.3769\n",
      "Epoch 82/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4918 - acc: 0.3898 - val_loss: 1.5192 - val_acc: 0.3757\n",
      "Epoch 83/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4916 - acc: 0.3902 - val_loss: 1.5185 - val_acc: 0.3737\n",
      "Epoch 84/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4914 - acc: 0.3909 - val_loss: 1.5194 - val_acc: 0.3752\n",
      "Epoch 85/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4912 - acc: 0.3912 - val_loss: 1.5186 - val_acc: 0.3749\n",
      "Epoch 86/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4911 - acc: 0.3910 - val_loss: 1.5188 - val_acc: 0.3755\n",
      "Epoch 87/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4909 - acc: 0.3909 - val_loss: 1.5202 - val_acc: 0.3785\n",
      "Epoch 88/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4907 - acc: 0.3920 - val_loss: 1.5193 - val_acc: 0.3760\n",
      "Epoch 89/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4906 - acc: 0.3919 - val_loss: 1.5195 - val_acc: 0.3755\n",
      "Epoch 90/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4904 - acc: 0.3928 - val_loss: 1.5196 - val_acc: 0.3716\n",
      "Epoch 91/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4903 - acc: 0.3921 - val_loss: 1.5198 - val_acc: 0.3771\n",
      "Epoch 92/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4900 - acc: 0.3920 - val_loss: 1.5194 - val_acc: 0.3764\n",
      "Epoch 93/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4900 - acc: 0.3927 - val_loss: 1.5196 - val_acc: 0.3733\n",
      "Epoch 94/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4898 - acc: 0.3936 - val_loss: 1.5202 - val_acc: 0.3738\n",
      "Epoch 95/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4897 - acc: 0.3931 - val_loss: 1.5197 - val_acc: 0.3749\n",
      "Epoch 96/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4896 - acc: 0.3929 - val_loss: 1.5199 - val_acc: 0.3750\n",
      "Epoch 97/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4894 - acc: 0.3932 - val_loss: 1.5195 - val_acc: 0.3757\n",
      "Epoch 98/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4893 - acc: 0.3933 - val_loss: 1.5199 - val_acc: 0.3778\n",
      "Epoch 99/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4891 - acc: 0.3932 - val_loss: 1.5208 - val_acc: 0.3757\n",
      "Epoch 100/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4889 - acc: 0.3943 - val_loss: 1.5202 - val_acc: 0.3750\n",
      "Epoch 101/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4887 - acc: 0.3939 - val_loss: 1.5212 - val_acc: 0.3800\n",
      "Epoch 102/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4888 - acc: 0.3945 - val_loss: 1.5199 - val_acc: 0.3784\n",
      "Epoch 103/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4887 - acc: 0.3941 - val_loss: 1.5202 - val_acc: 0.3767\n",
      "Epoch 104/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4885 - acc: 0.3939 - val_loss: 1.5200 - val_acc: 0.3764\n",
      "Epoch 105/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4883 - acc: 0.3945 - val_loss: 1.5203 - val_acc: 0.3795\n",
      "Epoch 106/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4881 - acc: 0.3952 - val_loss: 1.5209 - val_acc: 0.3739\n",
      "Epoch 107/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4882 - acc: 0.3945 - val_loss: 1.5200 - val_acc: 0.3774\n",
      "Epoch 108/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4880 - acc: 0.3953 - val_loss: 1.5207 - val_acc: 0.3767\n",
      "Epoch 109/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4879 - acc: 0.3953 - val_loss: 1.5199 - val_acc: 0.3772\n",
      "Epoch 110/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4877 - acc: 0.3945 - val_loss: 1.5202 - val_acc: 0.3774\n",
      "Epoch 111/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4876 - acc: 0.3953 - val_loss: 1.5214 - val_acc: 0.3779\n",
      "Epoch 112/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4875 - acc: 0.3964 - val_loss: 1.5205 - val_acc: 0.3785\n",
      "Epoch 113/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4874 - acc: 0.3964 - val_loss: 1.5207 - val_acc: 0.3789\n",
      "Epoch 114/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4873 - acc: 0.3964 - val_loss: 1.5215 - val_acc: 0.3743\n",
      "Epoch 115/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4872 - acc: 0.3956 - val_loss: 1.5210 - val_acc: 0.3782\n",
      "Epoch 116/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4870 - acc: 0.3973 - val_loss: 1.5212 - val_acc: 0.3765\n",
      "Epoch 117/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4869 - acc: 0.3959 - val_loss: 1.5209 - val_acc: 0.3764\n",
      "Epoch 118/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4869 - acc: 0.3958 - val_loss: 1.5206 - val_acc: 0.3784\n",
      "Epoch 119/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4868 - acc: 0.3961 - val_loss: 1.5209 - val_acc: 0.3769\n",
      "Epoch 120/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4867 - acc: 0.3965 - val_loss: 1.5215 - val_acc: 0.3777\n",
      "Epoch 121/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4865 - acc: 0.3960 - val_loss: 1.5213 - val_acc: 0.37900.39\n",
      "Epoch 122/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4865 - acc: 0.3964 - val_loss: 1.5227 - val_acc: 0.3790\n",
      "Epoch 123/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43226/43226 [==============================] - ETA: 0s - loss: 1.4870 - acc: 0.396 - 0s - loss: 1.4864 - acc: 0.3969 - val_loss: 1.5207 - val_acc: 0.3770\n",
      "Epoch 124/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4863 - acc: 0.3970 - val_loss: 1.5220 - val_acc: 0.3780\n",
      "Epoch 125/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4862 - acc: 0.3967 - val_loss: 1.5214 - val_acc: 0.3765\n",
      "Epoch 126/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4862 - acc: 0.3965 - val_loss: 1.5212 - val_acc: 0.3788\n",
      "Epoch 127/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4861 - acc: 0.3971 - val_loss: 1.5218 - val_acc: 0.3798\n",
      "Epoch 128/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4860 - acc: 0.3973 - val_loss: 1.5218 - val_acc: 0.3811\n",
      "Epoch 129/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4858 - acc: 0.3976 - val_loss: 1.5222 - val_acc: 0.3786\n",
      "Epoch 130/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4856 - acc: 0.3977 - val_loss: 1.5230 - val_acc: 0.3767\n",
      "Epoch 131/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4857 - acc: 0.3978 - val_loss: 1.5220 - val_acc: 0.3780\n",
      "Epoch 132/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4855 - acc: 0.3977 - val_loss: 1.5232 - val_acc: 0.3771\n",
      "Epoch 133/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4854 - acc: 0.3977 - val_loss: 1.5216 - val_acc: 0.3779\n",
      "Epoch 134/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4854 - acc: 0.3986 - val_loss: 1.5216 - val_acc: 0.3795\n",
      "Epoch 135/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4853 - acc: 0.3980 - val_loss: 1.5220 - val_acc: 0.3803\n",
      "Epoch 136/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4852 - acc: 0.3985 - val_loss: 1.5216 - val_acc: 0.3798\n",
      "Epoch 137/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4851 - acc: 0.3982 - val_loss: 1.5224 - val_acc: 0.3787\n",
      "Epoch 138/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4850 - acc: 0.3977 - val_loss: 1.5223 - val_acc: 0.3812\n",
      "Epoch 139/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4848 - acc: 0.3984 - val_loss: 1.5221 - val_acc: 0.3803\n",
      "Epoch 140/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4849 - acc: 0.3981 - val_loss: 1.5224 - val_acc: 0.3828\n",
      "Epoch 141/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4848 - acc: 0.3992 - val_loss: 1.5234 - val_acc: 0.3793\n",
      "Epoch 142/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4847 - acc: 0.3990 - val_loss: 1.5228 - val_acc: 0.3819\n",
      "Epoch 143/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4846 - acc: 0.3988 - val_loss: 1.5221 - val_acc: 0.3779\n",
      "Epoch 144/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4845 - acc: 0.3991 - val_loss: 1.5223 - val_acc: 0.3819\n",
      "Epoch 145/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4845 - acc: 0.3987 - val_loss: 1.5223 - val_acc: 0.3790\n",
      "Epoch 146/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4844 - acc: 0.3983 - val_loss: 1.5224 - val_acc: 0.3785\n",
      "Epoch 147/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4843 - acc: 0.3983 - val_loss: 1.5224 - val_acc: 0.3805\n",
      "Epoch 148/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4842 - acc: 0.3991 - val_loss: 1.5223 - val_acc: 0.3819\n",
      "Epoch 149/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4842 - acc: 0.3998 - val_loss: 1.5220 - val_acc: 0.3789\n",
      "Epoch 150/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4841 - acc: 0.4001 - val_loss: 1.5226 - val_acc: 0.3800\n",
      "Epoch 151/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4840 - acc: 0.3990 - val_loss: 1.5229 - val_acc: 0.3814\n",
      "Epoch 152/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4839 - acc: 0.3998 - val_loss: 1.5227 - val_acc: 0.3805\n",
      "Epoch 153/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4839 - acc: 0.4004 - val_loss: 1.5222 - val_acc: 0.3787\n",
      "Epoch 154/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4838 - acc: 0.4001 - val_loss: 1.5229 - val_acc: 0.3816\n",
      "Epoch 155/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4837 - acc: 0.4006 - val_loss: 1.5244 - val_acc: 0.3820\n",
      "Epoch 156/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4837 - acc: 0.3999 - val_loss: 1.5241 - val_acc: 0.3798\n",
      "Epoch 157/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4836 - acc: 0.3993 - val_loss: 1.5233 - val_acc: 0.3811\n",
      "Epoch 158/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4835 - acc: 0.4007 - val_loss: 1.5231 - val_acc: 0.3784\n",
      "Epoch 159/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4835 - acc: 0.4004 - val_loss: 1.5233 - val_acc: 0.3819\n",
      "Epoch 160/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4834 - acc: 0.4000 - val_loss: 1.5236 - val_acc: 0.3812\n",
      "Epoch 161/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4833 - acc: 0.4007 - val_loss: 1.5239 - val_acc: 0.3795\n",
      "Epoch 162/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4833 - acc: 0.4006 - val_loss: 1.5230 - val_acc: 0.3782\n",
      "Epoch 163/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4832 - acc: 0.4001 - val_loss: 1.5235 - val_acc: 0.3800\n",
      "Epoch 164/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4832 - acc: 0.4000 - val_loss: 1.5229 - val_acc: 0.3809\n",
      "Epoch 165/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4831 - acc: 0.4007 - val_loss: 1.5235 - val_acc: 0.3807\n",
      "Epoch 166/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4830 - acc: 0.4005 - val_loss: 1.5235 - val_acc: 0.3819\n",
      "Epoch 167/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4829 - acc: 0.4007 - val_loss: 1.5245 - val_acc: 0.3843\n",
      "Epoch 168/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4829 - acc: 0.4014 - val_loss: 1.5238 - val_acc: 0.3802\n",
      "Epoch 169/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4828 - acc: 0.4013 - val_loss: 1.5246 - val_acc: 0.3834\n",
      "Epoch 170/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4828 - acc: 0.4010 - val_loss: 1.5236 - val_acc: 0.3804\n",
      "Epoch 171/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4827 - acc: 0.4004 - val_loss: 1.5234 - val_acc: 0.3836\n",
      "Epoch 172/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4827 - acc: 0.4006 - val_loss: 1.5241 - val_acc: 0.3815\n",
      "Epoch 173/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4826 - acc: 0.4018 - val_loss: 1.5239 - val_acc: 0.3769\n",
      "Epoch 174/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4823 - acc: 0.4005 - val_loss: 1.5237 - val_acc: 0.3830\n",
      "Epoch 175/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4825 - acc: 0.4013 - val_loss: 1.5238 - val_acc: 0.3823\n",
      "Epoch 176/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4824 - acc: 0.4020 - val_loss: 1.5242 - val_acc: 0.3819\n",
      "Epoch 177/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4824 - acc: 0.4018 - val_loss: 1.5240 - val_acc: 0.3818\n",
      "Epoch 178/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4822 - acc: 0.4021 - val_loss: 1.5240 - val_acc: 0.3802\n",
      "Epoch 179/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4821 - acc: 0.4025 - val_loss: 1.5245 - val_acc: 0.3819\n",
      "Epoch 180/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4822 - acc: 0.4024 - val_loss: 1.5239 - val_acc: 0.3826\n",
      "Epoch 181/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4821 - acc: 0.4020 - val_loss: 1.5238 - val_acc: 0.3827\n",
      "Epoch 182/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4820 - acc: 0.4013 - val_loss: 1.5246 - val_acc: 0.3809\n",
      "Epoch 183/450\n",
      "43226/43226 [==============================] - 0s - loss: 1.4821 - acc: 0.4020 - val_loss: 1.5242 - val_acc: 0.3816\n",
      "Epoch 184/450\n",
      "27904/43226 [==================>...........] - ETA: 0s - loss: 1.4803 - acc: 0.4022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-517d20aa708d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                     validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1187\u001b[0m                         \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;31m# will be handled by on_epoch_end.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, current, values, force)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# Model of the exercice and plot\n",
    "####################################################################################################\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='softmax', input_shape=x_train.shape[1:]))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=450,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_history(history, 'Learning Curve and Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 8 : Determine the indices of all test images that are misclassified by the fitted model\n",
    "####################################################################################################\n",
    "\n",
    "##################################################\n",
    "# We found two ways of doing that\n",
    "##################################################\n",
    "\n",
    "####### First way #######\n",
    "prediction=model.predict(x_test, batch_size=None, verbose=0)\n",
    "idx=[]\n",
    "error=0\n",
    "for i in range (y_test[:,0].size):\n",
    "    for j in range (y_test[0,:].size):\n",
    "        if ((y_test[i][j]==1) & (max(prediction[i])!=prediction[i][j])):\n",
    "            idx.append(i)\n",
    "            error=error+1\n",
    "            break\n",
    "print ('First way : Number of errors: ',error)\n",
    "### plot some of them using the function ###\n",
    "plot_some_samples(x_test, y_test, prediction, idx[0:30]  ,label_mapping = subset_of_classes)\n",
    "plt.show()\n",
    "\n",
    "######## Second way ########\n",
    "idx_2=[]\n",
    "error_2=0\n",
    "for i in range (len(prediction)):\n",
    "    max_y_test=np.argmax(y_test[i])\n",
    "    max_pred=np.argmax(prediction[i])\n",
    "    if max_y_test!=max_pred :\n",
    "        idx_2.append(i)\n",
    "        error_2=error_2+1\n",
    "print ('Second way : Number of errors: ',error_2)\n",
    "### plot some of them using the function ###\n",
    "plot_some_samples(x_test, y_test, prediction, idx_2[0:30]  ,label_mapping = subset_of_classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 2 : Fit model with different Learning Rate\n",
    "####################################################################################################\n",
    "\n",
    "### Learning Rate ###\n",
    "l_r=[0.001,0.01,0.05,0.1,0.5,1]\n",
    "\n",
    "####################################################################################################\n",
    "# Simulation for the different learning rate\n",
    "####################################################################################################\n",
    "for i in range (len(l_r)):\n",
    "    model_1 = Sequential()\n",
    "    model_1.add(Dense(64, activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model_1.add(Dense(5, activation='softmax'))\n",
    "\n",
    "    # Inspect the model #\n",
    "    model_1.summary()\n",
    "\n",
    "    model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=l_r[i]),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history_1 = model_1.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=50,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "\n",
    "    score = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "    # plot #\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    plot_history(history_1, 'SGD (l_r={0}) : Learning Curve and Accuracy'.format(l_r[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) Very Large : The validation curve is sensitive to a noise. In fact with a huge learning rate, the set of validation data is not enough big to allow the validation curve to escape the noise. Moreover, we can see a difference in validation and training which is normal (we want to see that). Large learning rate will create an oscillation for the loss function around the minimum.  It is really not optimal because due to this large learning rate, the convergence will not be found. On the contrary, it can create a divergence. \n",
    "\n",
    "B) Very small : We can see that there are no difference between training and validation curve. In fact, with 50 epochs and a little learning rate, the model has not the time to learn well. The learning rate is small and thus as the changes between epochs will be too small, it will converge very slowly. We will need a huge number of epochs to converge to the best values and thus this is not optimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# SGD for the graph with lr=0.05 : Question 4\n",
    "####################################################################################################\n",
    "model_sgd = Sequential()\n",
    "model_sgd.add(Dense(64, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_sgd.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_sgd.summary()\n",
    "\n",
    "model_sgd.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "history_sgd = model_sgd.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_sgd.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_history(history_sgd, 'SGD l_r= 0.05 : Learning Curve and Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# ADAM OPTIMIZER Question 3 \n",
    "####################################################################################################\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(64, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_adam.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_adam.summary()\n",
    "\n",
    "model_adam.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "history_adam = model_adam.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=50,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_adam.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_history(history_adam, 'Adam : Learning Curve and Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 4: Graph of SGD and ADAM \n",
    "####################################################################################################\n",
    "\n",
    "##################################################\n",
    "# Plot with the 2 model\n",
    "##################################################\n",
    "def plot_history_2(history_adam,history_sgd, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history_adam.history['loss'], label = \"Training_loss_adam\")\n",
    "    ax1.plot(history_adam.history['val_loss'], label = \"Validation_loss_adam\")\n",
    "    ax2.plot(history_adam.history['acc'], label = \"Training_accuracy_adam\")\n",
    "    ax2.plot(history_adam.history['val_acc'], label = \"Validation_accuracy_adam\")\n",
    "    ax1.plot(history_sgd.history['loss'], label = \"Training_loss_sgd_lr=0.05\")\n",
    "    ax1.plot(history_sgd.history['val_loss'], label = \"Validation_loss_sgd_lr=0.05\")\n",
    "    ax2.plot(history_sgd.history['acc'], label = \"Training_accuracy_sgd_lr=0.05\")\n",
    "    ax2.plot(history_sgd.history['val_acc'], label = \"Validation_accuracy_sgd_lr=0.05\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n",
    "\n",
    "plot_history_2(history_adam,history_sgd,'Learning Curve and Accuracy for SGD and Adam optimizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# ADAM OPTIMIZER\n",
    "####################################################################################################\n",
    "model_adam = Sequential()\n",
    "model_adam.add(Dense(64, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_adam.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_adam.summary()\n",
    "\n",
    "model_adam.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "history_adam = model_adam.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_adam.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_history(history_adam, 'Adam optimizer : Loss and Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# EARLY STOPPING : ADAM OPTIMIZER\n",
    "####################################################################################################\n",
    "model_early = Sequential()\n",
    "model_early.add(Dense(64, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_early.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_early.summary()\n",
    "\n",
    "call= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "model_early.compile(loss='categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "history_early = model_early.fit(x_train, y_train,  callbacks=[call],\n",
    "                    batch_size=128,\n",
    "                    epochs=300,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_early.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "plot_history(history_early, 'Adam Optimizer with Early Stopping : Loss and Accuracy ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: The model start to overfit around the 50 epoch\n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**:  In statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\". An overfitted model is a statistical model that contains more parameters than can be justified by the data.The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.\n",
    "Thus we can see overfitting when the training accuracy inscrease since the validation accuracy stays the same. Moreover, we can see that overfitting begin when validation categorical cross entropy starts to increase after a rapid decrease since the training categorical cross entropy decrease. In fact theses two signs means that the training becomes more and more precise for the training set but it has no effect or bad effect on the validation set. It means that the model corresponds too closely or exactly to a particular set of data, the training data. This is overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 1 : Fit a reasonable number of models with different hidden layer size \n",
    "####################################################################################################\n",
    "\n",
    "#10#\n",
    "model_10 = Sequential()\n",
    "model_10.add(Dense(10, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_10.add(Dense(5, activation='softmax'))\n",
    "model_10.summary()\n",
    "model_10.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "          metrics=['accuracy'])\n",
    "history_10 = model_10.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_10.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#50#\n",
    "model_50 = Sequential()\n",
    "model_50.add(Dense(50, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_50.add(Dense(5, activation='softmax'))\n",
    "model_50.summary()\n",
    "model_50.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "          metrics=['accuracy'])\n",
    "history_50 = model_50.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model_50.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#100#\n",
    "model_100 = Sequential()\n",
    "model_100.add(Dense(100, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_100.add(Dense(5, activation='softmax'))\n",
    "model_100.summary()\n",
    "model_100.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "          metrics=['accuracy'])\n",
    "history_100 = model_100.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model_100.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#300#\n",
    "model_300 = Sequential()\n",
    "model_300.add(Dense(300, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_300.add(Dense(5, activation='softmax'))\n",
    "model_300.summary()\n",
    "model_300.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "          metrics=['accuracy'])\n",
    "history_300 = model_300.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model_300.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#600#\n",
    "model_600 = Sequential()\n",
    "model_600.add(Dense(600, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_600.add(Dense(5, activation='softmax'))\n",
    "model_600.summary()\n",
    "model_600.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.5),\n",
    "          metrics=['accuracy'])\n",
    "history_600 = model_600.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model_600.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#1000#\n",
    "model_1000 = Sequential()\n",
    "model_1000.add(Dense(1000, activation='relu', input_shape=x_train.shape[1:]))\n",
    "model_1000.add(Dense(5, activation='softmax'))\n",
    "model_1000.summary()\n",
    "model_1000.compile(loss='categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "          metrics=['accuracy'])\n",
    "history_1000 = model_1000.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model_1000.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "##################################################\n",
    "# Plot with all the curve of the different number of neuron in the hidden layer\n",
    "##################################################\n",
    "def plot_history_3(history_10,history_50,history_100,history_300,history_600,history_1000, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history_10.history['loss'], ':', color=\"blue\", label = \"Training Loss with 10 hidden neurons\")\n",
    "    ax1.plot(history_10.history['val_loss'], '--', color=\"blue\", label = \"Validation Loss with 10 hidden neurons\")\n",
    "    ax2.plot(history_10.history['acc'], ':', color=\"blue\",label = \"Training Accuracy with 10 hidden neurons\")\n",
    "    ax2.plot(history_10.history['val_acc'],'--', color=\"blue\", label = \"Validation Accuracy with 10 hidden neurons\")\n",
    "    ax1.plot(history_50.history['loss'], ':', color=\"red\",label = \"Training Loss with 50 hidden neurons\")\n",
    "    ax1.plot(history_50.history['val_loss'], '--', color=\"red\",label = \"Validation Loss with 50 hidden neurons\")\n",
    "    ax2.plot(history_50.history['acc'], ':', color=\"red\",label = \"Training Accuracy with 50 hidden neurons\")\n",
    "    ax2.plot(history_50.history['val_acc'], '--', color=\"red\",label = \"Validation Accuracy with 50 hidden neurons\")\n",
    "    ax1.plot(history_100.history['loss'], ':', color=\"green\",label = \"Training Loss with 100 hidden neurons\")\n",
    "    ax1.plot(history_100.history['val_loss'], '--', color=\"green\",label = \"Validation Loss with 100 hidden neurons\")\n",
    "    ax2.plot(history_100.history['acc'], ':', color=\"green\",label = \"Training Accuracy with 100 hidden neurons\")\n",
    "    ax2.plot(history_100.history['val_acc'], '--', color=\"green\",label = \"Validation Accuracy with 100 hidden neurons\")\n",
    "    ax1.plot(history_300.history['loss'], ':', color=\"orange\",label = \"Training Loss with 300 hidden neurons\")\n",
    "    ax1.plot(history_300.history['val_loss'], '--', color=\"orange\",label = \"Validation Loss with 300 hidden neurons\")\n",
    "    ax2.plot(history_300.history['acc'], ':', color=\"orange\",label = \"Training Accuracy with 300 hidden neurons\")\n",
    "    ax2.plot(history_300.history['val_acc'], '--', color=\"orange\",label = \"Validation Accuracy with 300 hidden neurons\")\n",
    "    ax1.plot(history_600.history['loss'], ':', color=\"yellow\",label = \"Training Loss with 600 hidden neurons\")\n",
    "    ax1.plot(history_600.history['val_loss'], '--', color=\"yellow\",label = \"Validation Loss with 600 hidden neurons\")\n",
    "    ax2.plot(history_600.history['acc'], ':', color=\"yellow\",label = \"Training Accuracy with 600 hidden neurons\")\n",
    "    ax2.plot(history_600.history['val_acc'], '--', color=\"yellow\",label = \"Validation Accuracy with 600 hidden neurons\")\n",
    "    ax1.plot(history_1000.history['loss'], ':', color=\"black\",label = \"Training Loss with 1000 hidden neurons\")\n",
    "    ax1.plot(history_1000.history['val_loss'], '--', color=\"black\",label = \"Validation Loss with 1000 hidden neurons\")\n",
    "    ax2.plot(history_1000.history['acc'], ':', color=\"black\",label = \"Training Accuracy with 1000 hidden neurons\")\n",
    "    ax2.plot(history_1000.history['val_acc'], '--', color=\"black\",label = \"Validation Accuracy with 1000 hidden neurons\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Plot all the model in the same plot\n",
    "##################################################\n",
    "plot_history_3(history_10,history_50,history_100,history_300,history_600,history_1000,'Learning Curve and Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 1 : You can see all the graph of the different network with the different numbers of hidden neuron \n",
    "####################################################################################################\n",
    "\n",
    "##################################################\n",
    "# Hiden Layer \n",
    "##################################################\n",
    "hidden=[10,50,100,300,600,1000]\n",
    "\n",
    "####################################################################################################\n",
    "# Simulation of the network for different number of hidden neuron \n",
    "####################################################################################################\n",
    "for i in range (len(hidden)):\n",
    "    model= Sequential()\n",
    "    model.add(Dense(hidden[i], activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "              metrics=['accuracy'])\n",
    "    history= model.fit(x_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=100,\n",
    "                        verbose=0,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    plot_history(history, 'Hidden Layer of {0} neurons : Learning Curve and Accuracy'.format(l_r[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 3 : Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. \n",
    "####################################################################################################\n",
    "import math\n",
    "\n",
    "start = time.time()\n",
    "print(\"Start of the script execution...\")\n",
    "\n",
    "##################################################\n",
    "# Hiden Layer \n",
    "##################################################\n",
    "hidden=[10,50,100,300,600,1000]\n",
    "\n",
    "##################################################\n",
    "# Number of simulation with different initial condtion\n",
    "##################################################\n",
    "times=5\n",
    "\n",
    "##################################################\n",
    "# Data for plot \n",
    "##################################################\n",
    "data_val_loss=np.empty((times,len(hidden)))\n",
    "data_val_accuracy=np.empty((times,len(hidden)))\n",
    "data_train_loss=np.empty((times,len(hidden)))\n",
    "data_train_acc=np.empty((times,len(hidden)))\n",
    "\n",
    "####################################################################################################\n",
    "# Model with different number of hidden neuron and with different initial condtition\n",
    "####################################################################################################\n",
    "\n",
    "# To have different initial conditions, we need to remake a model, and thanks to the seed, initial condtions would had change #\n",
    "for i in range (len(hidden)):\n",
    "        for k in range (times):\n",
    "            print('hidden =',hidden[i],' and k =',k)\n",
    "            model= Sequential()\n",
    "            model.add(Dense(hidden[i], activation='relu', input_shape=x_train.shape[1:], name='layer1'))\n",
    "            model.add(Dense(5, activation='softmax', name='layer2'))\n",
    "            model.summary()\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "                      metrics=['accuracy'])\n",
    "            history= model.fit(x_train, y_train,\n",
    "                                batch_size=128,\n",
    "                                epochs=250,\n",
    "                                verbose=0,\n",
    "                                validation_data=(x_test, y_test))\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            data_val_loss[k][i]= (history.history['val_loss'][-1])\n",
    "            data_val_accuracy[k][i]=(history.history['val_acc'][-1])\n",
    "            data_train_loss[k][i]=(history.history['loss'][-1])\n",
    "            data_train_acc[k][i]=(history.history['acc'][-1])\n",
    "\n",
    "print(\"End of the execution !\")\n",
    "end = time.time()\n",
    "print(\"Execution time : {} seconds.\".format(math.ceil(end - start)))\n",
    "\n",
    "##################################################\n",
    "# Plot \n",
    "##################################################\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_loss)\n",
    "plt.title('Boxplot : Validation Loss Plot')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_accuracy)\n",
    "plt.title('Boxplot : Validation Accuracy Plot')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# Question 4 : Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "####################################################################################################\n",
    "\n",
    "import math\n",
    "\n",
    "start = time.time()\n",
    "print(\"Start of the script execution...\")\n",
    "\n",
    "### Hiden Layer ###\n",
    "hidden=[10,50,100,300,600,1000]\n",
    "\n",
    "### Number of simulation with different initial condtion ###\n",
    "times=5\n",
    "\n",
    "### Data for plot ###\n",
    "data_val_loss=np.empty((times,len(hidden)))\n",
    "data_val_accuracy=np.empty((times,len(hidden)))\n",
    "data_train_loss=np.empty((times,len(hidden)))\n",
    "data_train_acc=np.empty((times,len(hidden)))\n",
    "\n",
    "\n",
    "### Model with different number of hidden neuron and with different initial condtition ###\n",
    "for i in range (len(hidden)):\n",
    "    for k in range(times):\n",
    "        print('hidden =',hidden[i],' and k =',k)\n",
    "        model= Sequential()\n",
    "        model.add(Dense(hidden[i], activation='relu', input_shape=x_train.shape[1:]))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        model.summary()\n",
    "        call= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=0, mode='auto')\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.05),\n",
    "                  metrics=['accuracy'])\n",
    "        history= model.fit(x_train, y_train, callbacks=[call],\n",
    "                            batch_size=128,\n",
    "                            epochs=250,\n",
    "                            verbose=0,\n",
    "                            validation_data=(x_test, y_test))\n",
    "        data_val_loss[k][i]= (history.history['val_loss'][-1])\n",
    "        data_val_accuracy[k][i]=(history.history['val_acc'][-1])\n",
    "        data_train_loss[k][i]=(history.history['loss'][-1])\n",
    "        data_train_acc[k][i]=(history.history['acc'][-1])\n",
    "        \n",
    "print(\"End of the execution !\")\n",
    "end = time.time()\n",
    "print(\"Execution time : {} seconds.\".format(math.ceil(end - start)))\n",
    "\n",
    "### Plot ###\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_loss)\n",
    "plt.title('Boxplot : Validation Loss Plot With Early Stopping')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_accuracy)\n",
    "plt.title('Boxplot : Validation Accuracy Plot With Early Stopping')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Accuracy')\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_train_loss)\n",
    "plt.title('Boxplot: Train Loss PLot With Early Stopping')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Train Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_train_acc)\n",
    "plt.title('Boxplot: Train Accuracy Plot With Early Stopping')\n",
    "plt.xticks([1,2,3,4,5,6], ['10','50','100','300','600','1000'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Train Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 1.Summary statistics of models with different number of hidden layer keeping neuron count same as 100 Hidden layer model\n",
    "#################################################################\n",
    "\n",
    "data_loss=[]\n",
    "data_accuracy=[]\n",
    "times=10;\n",
    "\n",
    "n_hidden = [100, 77, 66, 59]      # No of hidden neurons per layer\n",
    "n_repeat = [1, 2, 3, 4]           # No of hidden layers\n",
    "\n",
    "########################################\n",
    "# Create parametric model with given hidden and repeat arguments\n",
    "#########################################\n",
    "\n",
    "def dynamic_model(neurons, repeat):\n",
    "    y = Input(shape=x_train.shape[1:])\n",
    "    input_img = y\n",
    "    for j in range(repeat):\n",
    "        y = Dense(neurons, activation='relu') (y)\n",
    "    y = Dense(5, activation='softmax')(y)\n",
    "    model = Model(input_img, y)\n",
    "    return model\n",
    "\n",
    "models=[]\n",
    "\n",
    "for j in range(4):\n",
    "    print(\"\\n\\nModel Summary with {} hidden layers\".format(j))\n",
    "    models.append(dynamic_model(n_hidden[j], n_repeat[j]))    \n",
    "    models[j].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Part 2 - Running experiment for 5 times with different initial conditions for fixed 250 epochs\n",
    "###################################################\n",
    "\n",
    "\n",
    "n_hidden = [100, 77, 66, 59]\n",
    "n_repeat = [1, 2, 3, 4]\n",
    "total_exps = len(n_hidden)\n",
    "times = 5\n",
    "num_epochs = 500\n",
    "\n",
    "data_val_loss     = np.zeros((times,4))\n",
    "data_val_accuracy = np.zeros((times,4))\n",
    "data_train_loss   = np.zeros((times,4))\n",
    "data_train_acc    = np.zeros((times,4))\n",
    "\n",
    "summary_naive = {}\n",
    "\n",
    "\n",
    "######################################################\n",
    "# We dont use the early stopping criteria, however, we run for enough iterations and save the model with best validation accuracy\n",
    "# using the modelcheckpoint callback\n",
    "# In the output summary, you can see the values of best validation loss and accuracy\n",
    "########################################################\n",
    "\n",
    "for i in range(total_exps):\n",
    "    for k in range(times):\n",
    "        print(\"\\n\\n*****************************************************************************\")\n",
    "        print('Running experiment {}/{} : with {} hidden layers ({} neurons per hidden layer)'.format(k+1,times, i+1, n_hidden[i]))\n",
    "        model = dynamic_model(n_hidden[i], n_repeat[i])\n",
    "        model.summary()\n",
    "        #call= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=0, mode='auto')\n",
    "        saveBestModel = keras.callbacks.ModelCheckpoint(\"./best_weights.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto',save_weights_only=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(lr=0.05), metrics=['accuracy'])\n",
    "        history= model.fit(x_train, y_train, callbacks=[saveBestModel],batch_size=128, epochs=num_epochs, verbose=0, \n",
    "                           validation_data=(x_test, y_test))\n",
    "        model.load_weights(\"./best_weights.hdf5\")\n",
    "        score = model.evaluate(x_train, y_train, verbose=0)\n",
    "        print(\"Train Loss: {} , Train Accuracy: {}\".format(score[0], score[1]))\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        data_val_loss[k][i]= score[0]\n",
    "        data_val_accuracy[k][i]=score[1]\n",
    "        print(\"Valid Loss: {} , Valid Accuracy: {}\".format(data_val_loss[k][i],data_val_accuracy[k][i]))\n",
    "    summary_naive['hidden_{}'.format(n_repeat[i])] = history\n",
    "        \n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_loss)\n",
    "plt.title('Summary Statistics of Best Validation Loss')\n",
    "plt.xticks([1,2,3,4], ['1','2','3','4'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Loss')\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(data_val_accuracy)\n",
    "plt.title('Summary Statistics of Best Validation Accuracy')\n",
    "plt.xticks([1,2,3,4], ['1','2','3','4'])\n",
    "plt.xlabel('Number of hidden neurons')\n",
    "plt.ylabel('Final Validation Accuracy')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# adding regularisation to the model in the form of weight decay, dropout and\n",
    "# also batch normalization for reducing internal co-varaince shift\n",
    "##########################################################################\n",
    "\n",
    "n_hidden =[100, 59]\n",
    "n_repeat =[1, 4]\n",
    "l2_decay= 1e-4\n",
    "l1_decay= 1e-4\n",
    "\n",
    "#########################################################\n",
    "# Custom architecture with additional dropout, batchnorm\n",
    "#########################################################\n",
    "def dynamic_model_regularised(neurons, repeat, adddropout=True, addbatchnorm=True):\n",
    "    y = Input(shape=x_train.shape[1:])\n",
    "    input_img = y\n",
    "    for j in range(repeat):\n",
    "        if adddropout:\n",
    "            y = Dropout(0.1)(y)\n",
    "        y = Dense(neurons, activation=None, kernel_regularizer=regularizers.l2(l2_decay), bias_regularizer=regularizers.l1(l1_decay))(y)\n",
    "        if addbatchnorm:\n",
    "            y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "       \n",
    "    y = Dense(5, activation='softmax')(y)\n",
    "    model = Model(input_img, y)\n",
    "    return model\n",
    "\n",
    "models=[]\n",
    "for j in range(2):\n",
    "    print(\"\\n\\nModel Summary with {} hidden layers\".format(n_repeat[j]))\n",
    "    models.append(dynamic_model_regularised(n_hidden[j], n_repeat[j]))    \n",
    "    models[j].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Run model for multiple times and plot the summary statistics of best validation loss and accuracy\n",
    "##############################################################################\n",
    "import math\n",
    "\n",
    "lr_adam= 1e-3\n",
    "n_hidden =[100, 59]\n",
    "n_repeat =[1, 4]\n",
    "total_exps = len(n_hidden)\n",
    "times= 1\n",
    "n_exps= len(n_hidden)\n",
    "num_epochs = 500\n",
    "print(\" Running models with {} hidden layers for {} times\\n\\n\".format(', '.join([str(i) for i in range(len(n_hidden))]), times))\n",
    "\n",
    "data_val_loss     = np.zeros((times,n_exps))\n",
    "data_val_accuracy = np.zeros((times,n_exps))\n",
    "data_train_loss   = np.zeros((times,n_exps))\n",
    "data_train_acc    = np.zeros((times,n_exps))\n",
    "summary_reg={}\n",
    "\n",
    "\n",
    "####################################\n",
    "# Expriments are run for enough epochs and the best model is used to plot the summary statistics\n",
    "####################################\n",
    "\n",
    "for i in range(total_exps):\n",
    "    for k in range(times):\n",
    "        print(\"*****************************************************************************\")\n",
    "        print('\\n\\nRunning experiment {}/{} : with {} hidden layers ({} neurons per hidden layer)'.format(k+1,times, i+1, n_hidden[i]))\n",
    "        model=dynamic_model_regularised(n_hidden[i], n_repeat[i])\n",
    "        model.summary()\n",
    "        #call= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=15, verbose=0, mode='auto')\n",
    "        saveBestModel = keras.callbacks.ModelCheckpoint(\"./best_weights.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto',save_weights_only=True)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.005),metrics=['accuracy'])\n",
    "        history= model.fit(x_train, y_train, callbacks=[saveBestModel],\n",
    "                            batch_size=128, epochs=num_epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "        model.load_weights(\"./best_weights.hdf5\")\n",
    "        score = model.evaluate(x_train, y_train, verbose=0)\n",
    "        print(\"Train Loss: {} , Train Accuracy: {}\".format(score[0], score[1]))\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(\"Test loss: {} , Test accuracy: {}\".format(score[0],score[1]))\n",
    "      \n",
    "    summary_reg['hidden_{}'.format(n_repeat[i])] = history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "#Part 2 : plots of Regulairsed model vs Naive model\n",
    "####################################################\n",
    "title='Learning Curve and Accuracy of Regularised model vs Naive model (1 Hidden layer)'\n",
    "history_reg=summary_reg['hidden_1']\n",
    "history_naive=summary_naive['hidden_1']\n",
    "fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "ax1.plot(history_reg.history['loss'][:250], label = \"train reg\")\n",
    "ax1.plot(history_reg.history['val_loss'][:250], label = \"val reg\")\n",
    "ax1.plot(history_naive.history['loss'], label = \"train naive\")\n",
    "ax1.plot(history_naive.history['val_loss'], label = \"val naive\")\n",
    "\n",
    "\n",
    "ax2.plot(history_reg.history['acc'][:250], label = \"train reg.\")\n",
    "ax2.plot(history_reg.history['val_acc'][:250], label = \"vali reg.\")\n",
    "ax2.plot(history_naive.history['acc'], label = \"train naive\")\n",
    "ax2.plot(history_naive.history['val_acc'], label = \"val naive\")\n",
    "finalize_standardplot(fig, ax1, ax2)\n",
    "plt.show()\n",
    "\n",
    "title='Loss Curve and Accuracy of Regularised model vs Naive model (4 hidden layers)'\n",
    "history_reg=summary_reg['hidden_4']\n",
    "history_naive=summary_naive['hidden_4']\n",
    "fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "ax1.plot(history_reg.history['loss'][:250], label = \"train reg\")\n",
    "ax1.plot(history_reg.history['val_loss'][:250], label = \"val reg\")\n",
    "ax1.plot(history_naive.history['loss'], label = \"train naive\")\n",
    "ax1.plot(history_naive.history['val_loss'], label = \"val naive\")\n",
    "\n",
    "\n",
    "ax2.plot(history_reg.history['acc'][:250], label = \"train reg.\")\n",
    "ax2.plot(history_reg.history['val_acc'][:250], label = \"val reg.\")\n",
    "ax2.plot(history_naive.history['acc'], label = \"train naive\")\n",
    "ax2.plot(history_naive.history['val_acc'], label = \"val naive\")\n",
    "finalize_standardplot(fig, ax1, ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "#Plots of the Regularised model only\n",
    "#######################################################\n",
    "title='Learning Curve and Accuracy of Regularised (1 Hidden layer)'\n",
    "history_reg=summary_reg['hidden_1']\n",
    "history_naive=summary_naive['hidden_1']\n",
    "fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "ax1.plot(history_reg.history['loss'][:250], label = \"train reg\")\n",
    "ax1.plot(history_reg.history['val_loss'][:250], label = \"val reg\")\n",
    "\n",
    "\n",
    "ax2.plot(history_reg.history['acc'][:250], label = \"train reg.\")\n",
    "ax2.plot(history_reg.history['val_acc'][:250], label = \"vali reg.\")\n",
    "finalize_standardplot(fig, ax1, ax2)\n",
    "plt.show()\n",
    "\n",
    "title='Learning Curve and Accuracy of Regularised model (4 hidden layers)'\n",
    "history_reg=summary_reg['hidden_4']\n",
    "history_naive=summary_naive['hidden_4']\n",
    "fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "ax1.plot(history_reg.history['loss'][:250], label = \"train reg\")\n",
    "ax1.plot(history_reg.history['val_loss'][:250], label = \"val reg\")\n",
    "\n",
    "\n",
    "ax2.plot(history_reg.history['acc'][:250], label = \"train reg.\")\n",
    "ax2.plot(history_reg.history['val_acc'][:250], label = \"val reg.\")\n",
    "finalize_standardplot(fig, ax1, ax2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized  model doesnt seem to improve in spite of all kinds of dropout, regularizations etc.. We think reason being the naive model is already under-fitting  and further adding regularization wont help much. As a matter of fact, regularization will help if the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Run simple shallow model with a sequene of convolutional and pooling layers\n",
    "###############################################################################\n",
    "epochs = 250\n",
    "\n",
    "img_rows, img_cols = 16, 16\n",
    "\n",
    "x_train_ = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test_ = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train_ = x_train_.astype('float32')\n",
    "x_test_  = x_test_.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "l2_decay=1e-4\n",
    "l1_decay=1e-3\n",
    "lr_adam =1e-3\n",
    "\n",
    "w_reg = regularizers.l2(l2_decay)\n",
    "b_reg = regularizers.l1(l1_decay)\n",
    "summary_conv={}\n",
    "\n",
    "###########################\n",
    "# Simple shallow model \n",
    "#############################\n",
    "def conv_model():\n",
    "    Inp = Input(shape=input_shape)\n",
    "    y = Conv2D(32, kernel_size=(3, 3), activation='relu', kernel_regularizer=w_reg, bias_regularizer=b_reg)(Inp)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=w_reg, bias_regularizer=b_reg)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "    y = Conv2D(64, (3, 3), activation='relu', kernel_regularizer=w_reg, bias_regularizer=b_reg)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
    "    y = Dropout(0.25)(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(128, activation='relu',kernel_regularizer=w_reg, bias_regularizer=b_reg)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(5, activation='softmax',kernel_regularizer=w_reg, bias_regularizer=b_reg)(y)\n",
    "    model = Model(Inp,y)\n",
    "    return model\n",
    "\n",
    "model=conv_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5, verbose=0, mode='auto')\n",
    "saveBestModel = keras.callbacks.ModelCheckpoint(\"./best_weights.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='auto',save_weights_only=True)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adam(lr=lr_adam, decay=1e-5), metrics=['accuracy'])\n",
    "history = model.fit(x_train_, y_train, batch_size = 128,callbacks=[saveBestModel], epochs=epochs, verbose=0, validation_data=(x_test_, y_test))\n",
    "\n",
    "summary_conv['shallow_conv_model'] =history\n",
    "model.load_weights(\"./best_weights.hdf5\")\n",
    "score = model.evaluate(x_train_, y_train, verbose=0)\n",
    "print(\"Train Loss: {} , Train Accuracy: {}\".format(score[0], score[1]))\n",
    "score = model.evaluate(x_test_, y_test, verbose=0)\n",
    "print(\"Test loss: {} , Test accuracy: {}\".format(score[0],score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#Part 2- plots of CNN vs deep models\n",
    "####################################\n",
    "\n",
    "title='Learning Curve and Accuracy of CNN vs Non-CNN'\n",
    "history=summary_conv['shallow_conv_model'] \n",
    "history_old_best= summary_naive['hidden_4']\n",
    "fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "ax1.plot(history.history['loss'], label = \"train CNN\")\n",
    "ax1.plot(history.history['val_loss'], label = \"val CNN\")\n",
    "ax1.plot(history_old_best.history['loss'][:250], label = \"train FC\")\n",
    "ax1.plot(history_old_best.history['val_loss'][:250], label = \"val FC\")\n",
    "\n",
    "\n",
    "ax2.plot(history.history['acc'], label = \"train CNN\")\n",
    "ax2.plot(history.history['val_acc'], label = \"val CNN\")\n",
    "ax2.plot(history_old_best.history['acc'][:250], label = \"train FC\")\n",
    "ax2.plot(history_old_best.history['val_acc'][:250], label = \"val FC\")\n",
    "\n",
    "finalize_standardplot(fig, ax1, ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
