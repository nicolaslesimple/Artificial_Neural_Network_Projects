{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from torch import FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "# Standardization\n",
    "########################################################################################################\n",
    "def standardize(X):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    for i in range (len(X[0,:])):\n",
    "        mean_x = X[:,i].mean()\n",
    "        std_x = X[:,i] .std()\n",
    "        for j in range (len(X)):\n",
    "            X[j,i] = X[j,i] - mean_x\n",
    "            X[j,i]  = X[j,i]  / std_x\n",
    "    return X\n",
    "\n",
    "########################################################################################################\n",
    "# Find indexes for Cross validation \n",
    "########################################################################################################\n",
    "def cross_validation(dim, folds_number):\n",
    "    index_permutation=[]\n",
    "    index_folds = []\n",
    "    nb_fold = int(dim/folds_number)\n",
    "    for i in range (dim):\n",
    "        index_permutation.append(i)\n",
    "    for i in range(folds_number):\n",
    "        start = i*nb_fold\n",
    "        end = min([(i+1)*nb_fold, dim])\n",
    "        index_folds.append(index_permutation[start:end])\n",
    "    return index_folds\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "# Normalization \n",
    "########################################################################################################\n",
    "def normalize(X):\n",
    "    # Find the min and max values for each column\n",
    " #   x_min=FloatTensor(X.shape[1])\n",
    " #   x_max=FloatTensor(X.shape[1])\n",
    " #   x_min[0] = min(X[:,0])\n",
    " #   x_min[1]= min(X[:,1])\n",
    " #   x_max[0] = max(X[:,0])\n",
    " #   x_max[1] = max(X[:,1])\n",
    "    # Normalize\n",
    "    #for x in X:\n",
    "     #   for j in range(X.shape[1]):\n",
    "      #      x[j] = (x[j]-x_min[j])/(x_max[j]-x_min[j])\n",
    "   # print (X)\n",
    "    ####### Normalisation colonne ####3\n",
    "  #  sum_=FloatTensor(X.shape[1])\n",
    "#    for i in range (len(X[:,0])):\n",
    "#        sum_[0]=sum_[0] + X[i,0]**2\n",
    "#        sum_[1]=sum_[1] + X[i,1]**2\n",
    "#    sum_[0]=math.sqrt(sum_[0])\n",
    "#    sum_[1]=math.sqrt(sum_[1])\n",
    "#    for x in X:\n",
    "#        for j in range(X.shape[1]):\n",
    "#            x[j] = x[j]/sum_[i]\n",
    "    ###### Normalisation vecteur de taille 2 ########\n",
    "    for x in X:\n",
    "        sum_=0\n",
    "        for i in range (len(x)):\n",
    "            sum_=sum_ + x[i]*x[i]\n",
    "        sum_=math.sqrt(sum_)\n",
    "        for j in range(2):\n",
    "            x[j] = x[j]/sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### changer nom variable initilisation ################3\n",
    "\n",
    "class NeuralNetwork:\n",
    "    ########################################################################################################\n",
    "    # Initialisation\n",
    "    ########################################################################################################\n",
    "    def __init__(self, n_input=None, n_output=None, list_hidden_neurons=None):\n",
    "        self.n_output = n_output  # number of classes for the output. In our case, a probability that correspond to 0 or 1\n",
    "        self.n_input = n_input  # number of dimensions of the input. In our case we have 2 : x and y.\n",
    "        self.list_hidden_neurons = list_hidden_neurons  # list that represent the number of hidden node in each layer.\n",
    "        self.network = self.network_creation() # Building of the network thanks to the function you can see below\n",
    "\n",
    "    #\n",
    "    # ATTENTION FOR NOW THERE ARE NO BIAS TERM\n",
    "    #\n",
    "    ########################################################################################################\n",
    "    # Creation of all the weights ot the nodes that makes the network\n",
    "    ########################################################################################################\n",
    "    def network_creation(self):\n",
    "        def layer_creation(nb_node_input, nb_node_output): # We define a internal function for one layer because we will use it for each layer\n",
    "            layer = list() # we create the list that will contain information for each node in the layer\n",
    "            for i in range(nb_node_output):\n",
    "                weights = list() # We create a vector corresponding to the weights for each node : if intput layer have 10 nodes, the vector will have a size of ten.\n",
    "                for j in range(nb_node_input):\n",
    "                    weights.append(random.random()) # We initialize the weights thanks to the random function which give a number between 0 and 1\n",
    "                layer.append({'weights': weights,'output': None,'delta': None}) # We add the weigths informatons to each node\n",
    "            return layer\n",
    "        \n",
    "        # Now we want to use the layer_creation function to create the network for all the layer : input layer, hidden and output layer\n",
    "        network = list() # This list will have each layer in memory with information of each node in it (weights,output,delta)\n",
    "        if len(self.list_hidden_neurons) == 0: # If the lenght is 0, it means that there are no hidden layers\n",
    "            network.append(layer_creation(self.n_input, self.n_output))\n",
    "        else: # else there are one or more hidden layers\n",
    "            network.append(layer_creation(self.n_input, self.list_hidden_neurons[0])) # input connection with first hidden layer\n",
    "            for i in range(1,len(self.list_hidden_neurons)):\n",
    "                network.append(layer_creation(self.list_hidden_neurons[i-1],self.list_hidden_neurons[i])) # connection between hidden layers\n",
    "            network.append(layer_creation(self.list_hidden_neurons[len(self.list_hidden_neurons) -1],self.n_output)) # connection between the last hidden layer and the output layer\n",
    "\n",
    "        return network # we return the object network which is a list of layer. A layer is a list of tuple with one tuple for each node. A tuple is made by 3 part with weghts vector, output and delta vector.\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Forward method : it allows us to create and update the node's output from the input and thus we save at each node the values we want \n",
    "    ########################################################################################################\n",
    "    def forward_method(self, data):\n",
    "        # ATTENTION no bias term for our activation\n",
    "        # We define the activation function because we will use it a lot in for loop\n",
    "        def activate(weights, inputs):\n",
    "            activation = 0.0\n",
    "            for i in range(len(weights)):\n",
    "                activation = activation + weights[i] * inputs[i] # The function only compute the sum of the different component of the vector weights times the component of the input vector  \n",
    "            return activation\n",
    "        \n",
    "        tmp_data = data \n",
    "        for layer in self.network: \n",
    "            output = list()\n",
    "            for node in layer: # We iterate on each node on each layer of the network\n",
    "                activation = activate(node['weights'], tmp_data) # We compute activation and apply transfer to it\n",
    "                node['output'] = self.tanh(activation) # We apply transfer function to it\n",
    "                output.append(node['output']) # We save our update on the ouput node and on the output list\n",
    "            tmp_data = output\n",
    "            \n",
    "        return tmp_data\n",
    "\n",
    "    \n",
    "    # AATTNTTTENTION We need to change the error and we need to normalize it\n",
    "    ########################################################################################################\n",
    "    # Backward Method to take in account the error\n",
    "    ########################################################################################################\n",
    "    def backward_method(self, target):\n",
    "\n",
    "        # Perform backward-pass through network to update node deltas\n",
    "        n_layers = len(self.network)\n",
    "        for i in reversed(range(n_layers)):\n",
    "            layer = self.network[i]\n",
    "\n",
    "            # Compute errors either:\n",
    "            # - explicit target output difference on last layer\n",
    "            # - weights sum of deltas from frontward layers\n",
    "            errors = list()\n",
    "            if i == n_layers - 1:\n",
    "                # Last layer: errors = target output difference\n",
    "                for j, node in enumerate(layer):\n",
    "                    error = target[j] - node['output']\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                # Previous layers: error = weights sum of frontward node deltas\n",
    "                for j, node in enumerate(layer):\n",
    "                    error = 0.0\n",
    "                    for node in self.network[i + 1]:\n",
    "                        error += node['weights'][j] * node['delta']\n",
    "                    errors.append(error)\n",
    "\n",
    "            # Update delta using our errors\n",
    "            # The weight update will be:\n",
    "            # dW = learning_rate * errors * transfer' * input\n",
    "            #    = learning_rate * delta * input\n",
    "            for j, node in enumerate(layer):\n",
    "                node['delta'] = errors[j] * self.tanh_derivation(node['output'])\n",
    "\n",
    "                \n",
    "    ########################################################################################################\n",
    "    # Updating of the weights to reach the optimal weights thanks to the errror\n",
    "    ########################################################################################################\n",
    "    def update_method_weights(self, x, learning_rate=0.05):\n",
    "\n",
    "        # Update weights forward layer by layer\n",
    "        for i_layer, layer in enumerate(self.network):\n",
    "\n",
    "            # Choose previous layer output to update current layer weights\n",
    "            if i_layer == 0:\n",
    "                inputs = x\n",
    "            else:\n",
    "                inputs = FloatTensor(len(self.network[i_layer - 1])).zero_() # -1 prend en compte l'input layer\n",
    "                for i_node, node in enumerate(self.network[i_layer - 1]):\n",
    "                    inputs[i_node] = node['output']\n",
    "\n",
    "            # Update weights using delta rule for single layer neural network\n",
    "            # The weight update will be:\n",
    "            # dW = learning_rate * errors * transfer' * input\n",
    "            #    = learning_rate * delta * input\n",
    "            for node in layer:\n",
    "                for j, input in enumerate(inputs):\n",
    "                    dW = learning_rate * node['delta'] * input\n",
    "                    node['weights'][j] += dW\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Sigmoid Transfer Function \n",
    "    ########################################################################################################\n",
    "    def sigmoid(self,input_values):\n",
    "        return 1.0/(1.0+math.exp(-input_values))\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Softmax Transfer Function \n",
    "    ########################################################################################################\n",
    "    def softmax(self,input_values):\n",
    "        logits_exp = input_values.exp()\n",
    "        return logits_exp / torch.reduce_sum(logits_exp, 1)\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Hyperbolique Tangente Transfer Function \n",
    "    ########################################################################################################\n",
    "    def tanh(self,input_values):\n",
    "        return math.tanh(input_values)\n",
    "        \n",
    "    ########################################################################################################\n",
    "    # Hyperbolique Tangente Derivative Transfer Function\n",
    "    ########################################################################################################\n",
    "    def tanh_derivation(self,input_values):\n",
    "        return 1-(input_values**2)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Sigmoid Derivative Transfer Function\n",
    "    ########################################################################################################\n",
    "    def sigmoid_derivative(self, input_values):\n",
    "        return input_values*(1.0-input_values)\n",
    "     \n",
    "     #   A CHANGER CAR BESOIN DE NP\n",
    "    ########################################################################################################\n",
    "    # Cross Entropy Loss Function\n",
    "    ########################################################################################################\n",
    "    def cross_entropy_softmax_loss_array(softmax_input, y_label):\n",
    "        indices = np.argmax(y_label, axis = 1)#.astype(int)\n",
    "        predicted_probability = softmax_input[np.arange(len(softmax_input)), indices]\n",
    "        log_preds = np.log(predicted_probability)\n",
    "        loss = -1.0 * np.sum(log_preds) / len(log_preds)\n",
    "        return loss\n",
    "    \n",
    "         #   A CHANGER CAR BESOIN DE NP ATTENTIONNNNNNNNNNNNNNNNNNNNN\n",
    "    ########################################################################################################\n",
    "    # l2 REGULARIZATION FUNCTION\n",
    "    ########################################################################################################\n",
    "    def regularization_L2_softmax_loss(reg_lambda, weight1, weight2):\n",
    "        weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)\n",
    "        weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)\n",
    "        return weight1_loss + weight2_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########################################################################################################\n",
    "    # Train the model\n",
    "    ########################################################################################################\n",
    "    def _train(self,train_values, label, number_epochs=500, learning_rate=0.05,):\n",
    "        for epoch in range(number_epochs):\n",
    "            for (x, l) in zip(train_values, label):\n",
    "                self.forward_method(x) # Forward method to update the node\n",
    "                target = FloatTensor(self.n_output).zero_() # Create the label thanks to the real label we have as input which is y_train\n",
    "                target[int(l)] = 1 # If label is 0, y_target is [1,0] and if label is 1, y_target is [0,1]\n",
    "                self.backward_method(target)# Backward method that will take into account the error\n",
    "                self.update_method_weights(x, learning_rate=learning_rate)  # Update the weights thanks to the update method\n",
    "\n",
    "    ########################################################################################################\n",
    "    # Predcit the output with the forward method after the training\n",
    "    ########################################################################################################\n",
    "    def _predict_the_output(self, data):\n",
    "        prediction = FloatTensor(len(data)).zero_()\n",
    "        for j, x in enumerate(data): # We enumerate on all inputs \n",
    "            output = self.forward_method(x)  # Create the probability that makes the output\n",
    "            if output[0]<output[1]:\n",
    "                prediction[j] = 1  # We check which class is the highest one and thus we define our output\n",
    "            else : \n",
    "                prediction[j]=0\n",
    "        return prediction # We return the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    real_data=FloatTensor(1000,2).uniform_()\n",
    "    data= FloatTensor(1000,1)\n",
    "    label=FloatTensor(1000,1)\n",
    "    for i in range (len(real_data)):\n",
    "        if math.sqrt(real_data[i,0]**2 + real_data[i,1]**2)<(1/math.sqrt(2*math.pi)):\n",
    "            label[i]= 1\n",
    "        else :\n",
    "            label[i]= 0\n",
    "        data[i]=(real_data[i,0]**2 + real_data[i,1]**2)\n",
    "\n",
    "    real_data_test=FloatTensor(1000,2).uniform_()\n",
    "    label_test=FloatTensor(1000,1)\n",
    "    data_test=FloatTensor(1000,1)\n",
    "    for i in range (len(real_data_test)):\n",
    "        if math.sqrt(real_data_test[i,0]**2 + real_data_test[i,1]**2)<(1/math.sqrt(2*math.pi)):\n",
    "            label_test[i]= 1\n",
    "        else :\n",
    "            label_test[i]= 0\n",
    "        data_test[i]=(real_data_test[i,0]**2 + real_data_test[i,1]**2)\n",
    "    return real_data,label, real_data_test,label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model of our Neural Network:\n",
      "\n",
      " n_hidden_nodes = [25, 25, 25]\n",
      " number of folds = 4\n",
      " learning rate = 0.01\n",
      " number of epochs = 250\n",
      "\n",
      "Creating Data ...\n",
      " X.shape = torch.Size([1000, 2])\n",
      " y.shape = torch.Size([1000, 1])\n",
      " n_classes = 2\n",
      "\n",
      "Training and cross-validating...\n",
      " Fold 1/4: train acc = 38.40%, test acc = 35.20% (n_train = 750, n_test = 250)\n",
      " Fold 2/4: train acc = 35.87%, test acc = 39.20% (n_train = 750, n_test = 250)\n",
      " Fold 3/4: train acc = 37.87%, test acc = 34.80% (n_train = 750, n_test = 250)\n",
      " Fold 4/4: train acc = 62.40%, test acc = 66.40% (n_train = 750, n_test = 250)\n",
      "\n",
      "Avg train acc = 43.63%\n",
      "Avg test acc = 43.90%\n",
      "Test with new data : \n",
      "The Test Accuracy is :  81.3\n",
      "\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "  ⋮   \n",
      "    0\n",
      "    0\n",
      "    1\n",
      "[torch.FloatTensor of size 1000x1]\n",
      "\n",
      "\n",
      " 0\n",
      " 1\n",
      " 0\n",
      "⋮ \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 1000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # #######################################################################################\n",
    "    # Declaration of the parameters\n",
    "    # #######################################################################################\n",
    "    list_hidden_neurons = [25,25,25]  # number of neurons in each hidden layers\n",
    "    n_folds = 4  # number of folds we will use for our cross validation\n",
    "    l_r = 0.01  # learning rate\n",
    "    epochs = 250  # epochs\n",
    "   \n",
    "\n",
    "    print('Model of our Neural Network:')\n",
    "    print('\\n n_hidden_nodes = {}'.format(list_hidden_neurons))\n",
    "    print(\" number of folds = {}\".format(n_folds))\n",
    "    print(' learning rate = {}'.format(l_r))\n",
    "    print(\" number of epochs = {}\".format(epochs))\n",
    "   \n",
    "    \n",
    "\n",
    "    # #######################################################################################\n",
    "    # Read data (X,y) and normalize X\n",
    "    # #######################################################################################\n",
    "    print(\"\\nCreating Data ...\")\n",
    "    X,y,data_test,label_test=make_data()\n",
    "    #normalize(X)  # normalize\n",
    "    X =standardize(X)\n",
    "    dim, d = X.shape  # extract shape of X\n",
    "    n_classes = 2\n",
    "    \n",
    "    print(\" X.shape = {}\".format(X.shape))\n",
    "    print(\" y.shape = {}\".format(y.shape))\n",
    "    print(\" n_classes = {}\".format(n_classes))\n",
    "\n",
    "    # #######################################################################################\n",
    "    # Create cross-validation folds\n",
    "    # These are a list of a list of indices for each fold\n",
    "    # #######################################################################################\n",
    "    #idx_all = np.arange(0, N)\n",
    "    idx_all=[]\n",
    "    for i in range (dim):\n",
    "        idx_all.append(i)\n",
    "    idx_folds = cross_validation(dim, n_folds)\n",
    "\n",
    "    # #######################################################################################\n",
    "    # Train and evaluate the model on each fold\n",
    "    # #######################################################################################\n",
    "    acc_train, acc_test = list(), list()  # training/test accuracy score\n",
    "    print(\"\\nTraining and cross-validating...\")\n",
    "    for i, idx_test in enumerate(idx_folds):\n",
    "\n",
    "        # Collect training and test data from folds\n",
    "        #idx_train = np.delete(idx_all, idx_test)\n",
    "        idx_train = idx_all.copy()\n",
    "        counter=0\n",
    "        for j in range (len(idx_test)):\n",
    "            for h in range (len(idx_train)):\n",
    "                if (idx_train[h-counter] == idx_test[j]):\n",
    "                    idx_train.pop(h-counter)\n",
    "                    counter=counter+1\n",
    "        X_train, y_train = X[idx_train], y[idx_train]\n",
    "        X_test, y_test = X[idx_test], y[idx_test]\n",
    "        # Build neural network classifier model and train\n",
    "        model = NeuralNetwork(n_input=d, n_output=n_classes, list_hidden_neurons=list_hidden_neurons)\n",
    "        model._train(X_train, y_train, number_epochs=epochs, learning_rate=l_r,)\n",
    "\n",
    "        # Make predictions for training and test data\n",
    "        y_train_predict = model._predict_the_output(X_train)\n",
    "        y_test_predict = model._predict_the_output(X_test)\n",
    "\n",
    "        # #######################################################################################\n",
    "        # Compute accuracy for each training and testing set\n",
    "        # #######################################################################################\n",
    "        acc_train_counter=0\n",
    "        for j in range (len(y_train)):\n",
    "            if (int(y_train[j])==int(y_train_predict[j])):\n",
    "                acc_train_counter=acc_train_counter+1\n",
    "        acc_test_counter=0\n",
    "        for j in range (len(y_test)):\n",
    "            if (int(y_test[j])== int(y_test_predict[j])):\n",
    "                acc_test_counter=acc_test_counter+1\n",
    "                \n",
    "        # Compute training/test accuracy score from predicted values\n",
    "        acc_train.append(100*acc_train_counter/len(y_train))\n",
    "        acc_test.append(100*acc_test_counter/len(y_test))\n",
    "        # Print cross-validation result\n",
    "        print(\" Fold {}/{}: train acc = {:.2f}%, test acc = {:.2f}% (n_train = {}, n_test = {})\".format(i+1, n_folds, acc_train[-1], acc_test[-1], len(X_train), len(X_test)))\n",
    "    # #######################################################################################\n",
    "    # Print results\n",
    "    # #######################################################################################\n",
    "    print(\"\\nAvg train acc = {:.2f}%\".format(sum(acc_train)/float(len(acc_train))))\n",
    "    print(\"Avg test acc = {:.2f}%\".format(sum(acc_test)/float(len(acc_test))))\n",
    "    \n",
    "    \n",
    "    # #######################################################################################\n",
    "    # Test our network\n",
    "    # #######################################################################################\n",
    "    print ('Test with new data : ')\n",
    "    #  Make predictions for training and test data\n",
    "    y_test_predict_real = model._predict_the_output(data_test)\n",
    "    \n",
    "    #Accuracy\n",
    "    acc_test_real_counter=0\n",
    "    for j in range (len(label_test)):\n",
    "        if (int(label_test[j]) == int(y_test_predict_real[j])):\n",
    "            acc_test_real_counter=acc_test_real_counter+1\n",
    "    # Compute training/test accuracy score from predicted values\n",
    "    acc_test_real=(100*acc_test_real_counter/len(label_test))\n",
    "    print('The Test Accuracy is : ', acc_test_real)\n",
    "\n",
    "    print (label_test)\n",
    "    print (y_test_predict_real)\n",
    "\n",
    "\n",
    "# Driver\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-185-1485d0cf31b8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-185-1485d0cf31b8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    il faut coder la losss qui est l erreur pour faure une meilleure backpropagqation et aussi normaliser ne l2 la loss\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "il faut coder la losss qui est l erreur pour faure une meilleure backpropagqation et aussi normaliser ne l2 la loss\n",
    "je dois aussi rajouter les biais\n",
    "voir que faire entre normalisation et standardization\n",
    "ajouter aussi un temps de compilation et l afficher : voir miniprojet 1 de ann \n",
    "chercher comment initialiser les poids : serie 5\n",
    "rendre le code modulable pour l utilisateur\n",
    "afficher a chaque epoch la loss\n",
    "implementer sequential \n",
    "log tensor pour 1 et 0 \n",
    "coder softmax pour dernière layer\n",
    "\n",
    "checker cours pour initialisation des poids\n",
    "\n",
    "checker pour cross si je prend la moyeenne de tous mes paramètre données\n",
    "\n",
    "\n",
    "    Bonus : coder un droupout (voir ann projet 1) et batch normalization(l2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
